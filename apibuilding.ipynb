{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf03ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tds_discourse_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    discourse_data = json.load(f)\n",
    "\n",
    "discourse_chunks = []\n",
    "for item in discourse_data:\n",
    "    for post in item[\"posts\"]:\n",
    "        if post.strip():\n",
    "            discourse_chunks.append({\n",
    "                \"source\": f\"Discourse | {item['title']}\",\n",
    "                \"text\": post.strip()\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b6c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "course_chunks = []\n",
    "\n",
    "for filename in os.listdir(\"scraped_tds\"):  # your markdown folder\n",
    "    if filename.endswith(\".md\"):\n",
    "        with open(os.path.join(\"scraped_tds\", filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            course_chunks.append({\n",
    "                \"source\": f\"Course | {filename}\",\n",
    "                \"text\": text\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1bf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Use a local free embedding model from Hugging Face\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Split documents\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=chunk[\"text\"], metadata={\"source\": chunk[\"source\"]})\n",
    "    for chunk in (course_chunks + discourse_chunks)\n",
    "]\n",
    "\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# Vectorize and save\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "vectorstore.save_local(\"tds_vector_db\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
